{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood Analysis with Asymmetric Drift\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "sys.path.append(\"../gaia_tools/\")\n",
    "import data_analysis\n",
    "import transformation_constants\n",
    "import covariance_generation\n",
    "from import_functions import import_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/hdfs/local/sven/gaia_tools_data/gaia_rv_data_bayes.csv\"\n",
    "data_icrs = import_data(path = path, debug = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galcen_data = data_analysis.get_transformed_data(data_icrs,\n",
    "                                       include_cylindrical = True,\n",
    "                                       debug = True,\n",
    "                                       is_bayes = True,\n",
    "                                       is_source_included = True)\n",
    "\n",
    "cov_df = covariance_generation.generate_covmatrices(df = data_icrs,\n",
    "                                       df_crt = galcen_data,\n",
    "                                       transform_to_galcen = True,\n",
    "                                       transform_to_cylindrical = True,\n",
    "                                       is_bayes = True,\n",
    "                                       debug=False)\n",
    "\n",
    "# append covariance information to galactocentric data\n",
    "galcen_data['cov_mat'] = cov_df['cov_mat']\n",
    "\n",
    "galcen_data = galcen_data[(galcen_data.r < 12000) & (galcen_data.r > 5000)]\n",
    "galcen_data = galcen_data[(galcen_data.z < 200) & (galcen_data.z > -200)]\n",
    "galcen_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "data_icrs = data_icrs.merge(galcen_data, on='source_id')[data_icrs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIKELIHOOD SUM FUNCTION\n",
    "'''\n",
    "This function uses the Gaia data in ICRS:\n",
    "1) Transforms it into a Galactocentric frame using the theta arguments given\n",
    "2) Generates the covariance matrices (also transforms) and appends them to the Galactocentric data\n",
    "3) Bins the data and generates a 'BinCollection' object\n",
    "4) Iterates over the bins and computes a likelihood value for each\n",
    "5) Sums the likelihood values over the bins\n",
    "'''\n",
    "\n",
    "def get_likelihood_sum(data_icrs,\n",
    "                        vc_list,\n",
    "                       r = transformation_constants.R_0, \n",
    "                       z = transformation_constants.Z_0, \n",
    "                       Usun = transformation_constants.V_SUN[0][0],\n",
    "                       Vsun = transformation_constants.V_SUN[1][0],\n",
    "                       num_r_bin = 10,\n",
    "                       num_z_bin = 4):\n",
    " \n",
    "    \n",
    "    # theta = (r, z, Usun, Vsun, transformation_constants.V_SUN[2][0])\n",
    "    \n",
    "    # v_sun = np.array([[theta[2]], \n",
    "    #                           [theta[3]], \n",
    "    #                           [theta[4]]])\n",
    "    # # 1\n",
    "    # galcen_data = data_analysis.get_transformed_data(data_icrs, \n",
    "    #                                    include_cylindrical = True, \n",
    "    #                                    r_0 = theta[0],\n",
    "    #                                    v_sun = v_sun,\n",
    "    #                                    debug = True,\n",
    "    #                                    is_bayes = True, \n",
    "    #                                    is_source_included = True)\n",
    "    \n",
    "    # # 2\n",
    "    # cov_df = covariance_generation.generate_covmatrices(df = data_icrs, \n",
    "    #                                        df_crt = galcen_data, \n",
    "    #                                        transform_to_galcen = True, \n",
    "    #                                        transform_to_cylindrical = True,\n",
    "    #                                        z_0 = theta[1],\n",
    "    #                                        r_0 = theta[0],\n",
    "    #                                        is_bayes = True,\n",
    "    #                                        debug=True)\n",
    "    \n",
    "    # galcen_data['cov_mat'] = cov_df['cov_mat']\n",
    "    \n",
    "\n",
    "    # galcen_data = galcen_data[(galcen_data.r < 12000) & (galcen_data.r > 5000) ]\n",
    "    # galcen_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "    # min_val = np.min(galcen_data.r)\n",
    "    # max_val = np.max(galcen_data.r)\n",
    "    \n",
    "    \n",
    "    # # 3\n",
    "    # bin_collection = data_analysis.get_collapsed_bins(data = galcen_data, \n",
    "    #                                                              theta = theta, \n",
    "    #                                                              BL_r_min = min_val - 1, \n",
    "    #                                                              BL_r_max = max_val + 1, \n",
    "    #                                                              BL_z_min = -200, \n",
    "    #                                                              BL_z_max = 200, \n",
    "    #                                                              N_bins = (num_r_bin, num_z_bin ),\n",
    "    #                                                              r_drift = False, \n",
    "    #                                                              debug = True)\n",
    "\n",
    "\n",
    "    # Setup likelihood array\n",
    "    n = reduce(lambda x, y: x*y, bin_collection.N_bins)\n",
    "    likelihood_array = np.zeros(n)\n",
    "\n",
    "\n",
    "    # Keep track how many data points are used in likelihood computation\n",
    "    point_count = []\n",
    "    \n",
    "    # 4\n",
    "    start = time.time()\n",
    "    for i, bin in enumerate(bin_collection.bins):\n",
    "        \n",
    "        likelihood_value = bin.get_likelihood_w_asymmetry(vc_list[i], debug=True)\n",
    "        \n",
    "        if(likelihood_value == 0):\n",
    "            print(\"0!!\")\n",
    "            val = 0\n",
    "\n",
    "        else:\n",
    "            #print(bin.N_points)\n",
    "            point_count.append(bin.N_points)\n",
    "            \n",
    "            # get bin likelihood\n",
    "            val = likelihood_value\n",
    "\n",
    "            # convert chi-squared\n",
    "            #val = val*(-2)/star_count\n",
    "\n",
    "        likelihood_array[i] = val\n",
    "    \n",
    "    print(\"Number of points in analysis: {0}\".format(np.sum(point_count)))\n",
    "    print(\"Bin Collection data shape: {0}\".format(bin_collection.data.shape))\n",
    "    \n",
    "    likelihood_sum = np.sum(likelihood_array)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"Likelihood time = %s\" % (end - start))\n",
    "    \n",
    "    return likelihood_sum, bin_collection, likelihood_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that generates the neccessary variables for \n",
    "# plotting the profiles\n",
    "\n",
    "def generate_plot_vars(bin_r, bin_z, parameter, vc_list):\n",
    "    \n",
    "    # The varied range in x-axis    \n",
    "    x = vc_list\n",
    "    \n",
    "    # The likelihood values\n",
    "    y = []\n",
    "\n",
    "    for i, item in enumerate(x):\n",
    "        print(i, item)\n",
    "        \n",
    "        vc_list = [item for idx in range(0,11)]\n",
    "\n",
    "        val = get_likelihood_sum(data_icrs,\n",
    "                                vc_list,\n",
    "                                num_r_bin = bin_r,\n",
    "                                num_z_bin = bin_z)[0]\n",
    "        \n",
    "        print(\"Likelihood: {0}\".format(val))\n",
    "        y.append(val)\n",
    "\n",
    "    return x, y, parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plotting function\n",
    "\n",
    "def generate_likelihood_plot(x, y, bin_r, bin_z, parameter, save = False):\n",
    "    \n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    plt.plot(x, y, '-', color='blue')\n",
    "    plt.title(\"Likelihood dependence on ${0}$\".format(parameter), pad = 45, fontdict={'fontsize': 20})\n",
    "    plt.suptitle(r\"({0}x{1} bins)\".format(bin_r, bin_z), y=0.93, fontsize=15)\n",
    "    plt.grid()\n",
    "\n",
    "    idx_max = np.argmax(y)\n",
    "    plt.axvline(x=x[idx_max], ls=\"--\", label=\"Max\")\n",
    "\n",
    "\n",
    "    if(parameter == \"R_0\"):\n",
    "        unit = \"pc\"\n",
    "    else:\n",
    "        unit = \"km/s\"\n",
    "    \n",
    "    plt.xlabel('${0}$ [{1}]'.format(parameter, unit), fontdict={'fontsize': 18}, labelpad = 25)\n",
    "    plt.ylabel('Log Likelihood',fontdict={'fontsize': 18}, labelpad = 25)\n",
    "    plt.subplots_adjust(left=0.2)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    title_string = \"../out/Likelihood_{0}_{1}x{2}\".format(parameter, bin_r, bin_z)\n",
    "\n",
    "    if(save):\n",
    "        plt.savefig(title_string+'.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## $V_\\odot$ Likelihood\n",
    "$V_\\odot$ is varied, rest of the parameters are fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = \"V_odot\"\n",
    "\n",
    "bin_r = 5\n",
    "bin_z = 1\n",
    "\n",
    "vc_list = [item for item in range(-300, -150, 10)]\n",
    "x, y, parameter = generate_plot_vars(bin_r, bin_z, parameter, vc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_likelihood_plot(x, y, bin_r, bin_z, \"V_\\odot\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE PLOT\n",
    "generate_likelihood_plot(x, y, bin_r, bin_z, \"V_\\odot\")\n",
    "\n",
    "# Reid & Brunthaler + GRAVITY: 245.6 +- 1.4 [km/s]\n",
    "v_circ = 233.4\n",
    "# plt.axvline(x=v_circ, label=\"Reid & Brunthaler + GRAVITY\\n (2020)\", color=\"orange\")\n",
    "# plt.axvline(x=v_circ+1.5, ls=\"--\", color=\"orange\")\n",
    "# plt.axvline(x=v_circ-1.5, ls=\"--\", color=\"orange\")\n",
    "\n",
    "#plt.xlim(175, 325)\n",
    "plt.legend()\n",
    "#plt.savefig('../out/Vodot_profile'+'.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
